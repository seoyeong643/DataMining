{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRqYpnKs743V"
      },
      "source": [
        "# CSE 572: Lab 6\n",
        "\n",
        "In this lab, you will practice implementing feed-forward network and parameter tuning with k-fold cross valivation.\n",
        "\n",
        "To execute and make changes to this notebook, click File > Save a copy to save your own version in your Google Drive or Github. Read the step-by-step instructions below carefully. To execute the code, click on each cell below and press the SHIFT-ENTER keys simultaneously or by clicking the Play button.\n",
        "\n",
        "When you finish executing all code/exercises, save your notebook then download a copy (.ipynb file). Submit the following **three** things:\n",
        "1. a link to your Colab notebook,\n",
        "2. the .ipynb file, and\n",
        "3. a pdf of the executed notebook on Canvas.\n",
        "\n",
        "To generate a pdf of the notebook, check the instructions on Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PUT YOUR GROUP INFO HERE**\n",
        "\n",
        "| Group number | August Group XXX |            |\n",
        "|--------------|------------------|------------|\n",
        "| Member 1     | NAME             | ASURITE ID |\n",
        "| Member 2     |                  |            |\n",
        "| Member 3     |                  |            |\n",
        "| Member 4     |                  |            |"
      ],
      "metadata": {
        "id": "TQK0j72eAUVe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHBWV1zY743X"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "seed = 0\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKRajBP6743Y"
      },
      "source": [
        "## Feed-forward neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yd3tZZAL743Y"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "For this example, we will use the [Cleveland Heart Disease dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease). Review the dataset documentation to learn more about the attributes and other aspects of the dataset. The dataset consists of a CSV file with 303 rows. Each row contains information about a patient. There are 14 attribute columns and one binary class column (`target`) that reports whether or not a patient had a heart disease. We will train a feed-forward neural network model to predict whether or not a given patient has a heart disease based on the attribute values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqCBU86O743Y"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv(\"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yfcwo-wE743Y"
      },
      "outputs": [],
      "source": [
        "# Print sample rows\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9imJZYz743Z"
      },
      "outputs": [],
      "source": [
        "# Print the number of rows and columns\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSMf2ZKJ743Z"
      },
      "source": [
        "Split the dataset into three randomly-sampled subsets: training (60%), validation (20%), and test (20%). Use the `seed` variable for the `random_state`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOLA1SDt743Z"
      },
      "outputs": [],
      "source": [
        "train, val, test = np.split(data.sample(frac=1, random_state=seed), [int(.6*len(data)), int(.8*len(data))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkQkrbDY743Z"
      },
      "source": [
        "Print the number of samples in each of the three subsets and the number of instances from each class. For example, for the training set you might print \"The training set has __ instances (__ heart disease, __ no disease)\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYDvfC9B743Z"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyyJWGEH743a"
      },
      "source": [
        "### Prepare the dataset\n",
        "\n",
        "Before we can feed this dataset to our model for training and evaluation, we need to perform a few steps to get it ready:\n",
        "1. Convert the dataframes to Dataset objects\n",
        "2. Normalize the numerical feature values\n",
        "3. Binarize the Categorical features by converting to one-hot encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leOGCBGc743a"
      },
      "outputs": [],
      "source": [
        "# Convert dataframes to Dataset objects\n",
        "def dataframe_to_dataset(df, shuffle=True):\n",
        "    df = df.copy()\n",
        "    # Remove the target column and store in a separate array\n",
        "    labels = df.pop('target')\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(buffer_size=len(df), reshuffle_each_iteration=False)\n",
        "\n",
        "    return ds\n",
        "\n",
        "\n",
        "train_ds_original = dataframe_to_dataset(train)\n",
        "val_ds_original = dataframe_to_dataset(val)\n",
        "test_ds_original = dataframe_to_dataset(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfEphi8B743a"
      },
      "source": [
        "The Dataset object yields a tuple containing the input feature vector and target (class value): `(input, target)`. `input` is a dictionary of features and `target` is the value 0 or 1. The code below prints an example instance drawn from the training Dataset object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4Pu2C14743a"
      },
      "outputs": [],
      "source": [
        "for x, y in train_ds_original.take(1):\n",
        "    print(\"Input:\", x)\n",
        "    print(\"\\nTarget:\", y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrI2oY2W743a"
      },
      "source": [
        "We can use the batch() function in keras to create batches from the full dataset for passing to the model. For the training dataset, we'll define a hyperparameter `batch_size` that we will set. For the validation and test sets, we will make the batch size equivalent to the size of the subset so all samples in that subset are evaluated each time the dataset is evaluated by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9tEQ4IC743a"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_ds = train_ds_original.batch(batch_size)\n",
        "val_ds = val_ds_original.batch(val.shape[0])\n",
        "test_ds = test_ds_original.batch(test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43E2kq7C743a"
      },
      "source": [
        "There are seven categorical features in the dataset: `sex`, `cp`, `fbs`, `restecg`, `exang`, `ca`, and `thal`. You can read more about what these features mean in the [dataset documentation](https://archive.ics.uci.edu/ml/datasets/heart+Disease). All of them except `thal` have integer data type while `thal` has String data type. Below we define a function to encode these feature values as one-hot encodings using the [IntegerLookup()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/IntegerLookup) and [StringLookup()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) layers. These layers create look-up tables for mapping a set of arbitrary integers or strings to a one-hot encoding. We use an `is_string` argument to indicate whether we should use the `StringLookup()` for `thal` or the `IntegerLookup()` for the remaining features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgz2Jq0w743a"
      },
      "outputs": [],
      "source": [
        "def encode_categorical_feature(feature, name, dataset, is_string):\n",
        "    from tensorflow.keras.layers import IntegerLookup\n",
        "    from tensorflow.keras.layers import StringLookup\n",
        "\n",
        "    # Create lookup layer to turn categorical features into 1-hot integer encodings\n",
        "    if is_string:\n",
        "        lookup = StringLookup(output_mode=\"binary\")\n",
        "    else:\n",
        "        lookup = IntegerLookup(output_mode=\"binary\")\n",
        "\n",
        "    # Prepare a Dataset that only yields the feature of interest\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "\n",
        "    # Find the set of possible values and assign them a fixed integer index\n",
        "    lookup.adapt(feature_ds)\n",
        "\n",
        "    # Turn the input into integer indices\n",
        "    encoded_feature = lookup(feature)\n",
        "    return encoded_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJzZfDI-743a"
      },
      "source": [
        "The remaining features in the dataset (`age`, `trestbps`, `chol`, `thalach`, `oldpeak`, and `slope`) are all numerical measurements. You can read more about what these features mean in the [dataset documentation](https://archive.ics.uci.edu/ml/datasets/heart+Disease). We don't need to encode the numerical features, but we do want to scale them to the same range of values (e.g., using standardization or normalization). Below we define a function that uses the [Normalization()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Normalization) layer to standardize the data (subtract the mean and divide by the standard deviation for each feature)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuoNOgqR743a"
      },
      "outputs": [],
      "source": [
        "def normalize(feature, name, dataset):\n",
        "    from tensorflow.keras.layers import Normalization\n",
        "\n",
        "    # Create a Normalization layer for our feature\n",
        "    normalizer = Normalization()\n",
        "\n",
        "    # Prepare a Dataset that only yields the feature of interest\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "\n",
        "    # Learn the statistics of the data\n",
        "    normalizer.adapt(feature_ds)\n",
        "\n",
        "    # Normalize the input feature\n",
        "    norm_feature = normalizer(feature)\n",
        "    return norm_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2tyxoZI743a"
      },
      "source": [
        "Now we can apply these functions to each of our features and return an encoded/preprocessed input layer. We first create tensor variables for each of the inputs, then apply the appropriate function, then concatenate all of these input layers for each feature together to form a single input feature layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqVxcA4z743a"
      },
      "outputs": [],
      "source": [
        "all_inputs = [\n",
        "    keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"thal\", dtype=\"string\"),\n",
        "    keras.Input(shape=(1,), name=\"age\"),\n",
        "    keras.Input(shape=(1,), name=\"trestbps\"),\n",
        "    keras.Input(shape=(1,), name=\"chol\"),\n",
        "    keras.Input(shape=(1,), name=\"thalach\"),\n",
        "    keras.Input(shape=(1,), name=\"oldpeak\"),\n",
        "    keras.Input(shape=(1,), name=\"slope\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gVJp-lo743a"
      },
      "outputs": [],
      "source": [
        "feature_layer = layers.concatenate(\n",
        "    [\n",
        "        encode_categorical_feature(all_inputs[0], \"sex\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[1], \"cp\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[2], \"fbs\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[3], \"restecg\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[4], \"exang\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[5], \"ca\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[6], \"thal\", train_ds, True),\n",
        "        normalize(all_inputs[7], \"age\", train_ds),\n",
        "        normalize(all_inputs[8], \"trestbps\", train_ds),\n",
        "        normalize(all_inputs[9], \"chol\", train_ds),\n",
        "        normalize(all_inputs[10], \"thalach\", train_ds),\n",
        "        normalize(all_inputs[11], \"oldpeak\", train_ds),\n",
        "        normalize(all_inputs[12], \"slope\", train_ds)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq6lw5Mp743b"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "Now that we've prepared our dataset, we can construct our neural network model. We construct the model by composing Layer objects starting with the input layer (which we've already defined as `feature_layer`) and ending with the output layer (which will be the final output of the model). In this example, we will only use [Dense()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers which are simple fully-connected feed-forward layers (i.e., each output from layer `i-1` is connected by a weight variable to every neuron in layer `i`). We will create a network with only one hidden layer (between the input and output layers).\n",
        "\n",
        "The Dense() layer object allows us to specify the activation function to use using the `activation` argument. In class, we talked about several activation functions including sigmoid, sign, and tanh. Another commonly used activation function is the rectified linear unit, or \"ReLU\" function. Another commonly used activation function is the rectified linear unit, or \"ReLU\" function, which has the equation $a(z)=max(0,z)$. We will use `relu` as our activation function in this example for all layers except the final layer, which will use a `sigmoid` activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn55XKtZ743b"
      },
      "outputs": [],
      "source": [
        "# Create a variable for the number of units/neurons in the layer\n",
        "h1_units = 32\n",
        "# Create a Dense layer and append it to the input layer\n",
        "h1_layer = layers.Dense(h1_units, activation='relu')(feature_layer)\n",
        "\n",
        "# Create an output layer with one output representing the likelihood of\n",
        "# heart disease and append it to the hidden layer\n",
        "output_layer = layers.Dense(1, activation=\"sigmoid\")(h1_layer)\n",
        "\n",
        "# Build the model specifying the input and output layer\n",
        "model = keras.Model(inputs=all_inputs, outputs=output_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HD50sRJ743b"
      },
      "source": [
        "We can plot our completed model to visualize the input, hidden, and output layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrxLxujr743b"
      },
      "outputs": [],
      "source": [
        "# `rankdir='LR'` is to make the graph horizontal\n",
        "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78eEEGzH743b"
      },
      "source": [
        "Next we compile the model by specifying the optimization technique and loss function to be used in model training. We can also specify the metric(s) that will be logged during training. We will use stochastic gradient descent (`sgd`) for the optimizer and binary cross entropy (log loss) as the loss function. We will log the accuracy metric during training. We can also specify the learning rate hyperparameter here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mavh5BmH743b"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "model.compile(keras.optimizers.SGD(learning_rate=learning_rate), \"binary_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e05BSxE-743b"
      },
      "source": [
        "### Train the model\n",
        "\n",
        "Now that we've constructed and compiled our model, we can train the model using our training dataset. This is done in keras using the `fit()` function, which also gives us an option to provide the validation dataset which will be used to evaluate validation accuracy after every epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY7N2oVX743b"
      },
      "outputs": [],
      "source": [
        "model_result = model.fit(train_ds, epochs=100, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLKQHP1k743b"
      },
      "source": [
        "The `fit()` function returns a history attribute that gives the metrics recorded during training as a dictionary. We can print the dictionary keys to see which metrics were stored:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDI4XAkI743b"
      },
      "outputs": [],
      "source": [
        "model_result.history.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRE2MPRW743b"
      },
      "source": [
        "Create a figure with two subplots. The first subplot should plot the training and validation loss (`loss` and `val_loss`) and the second subplot should plot the training and validation accuracy (`accuracy` and `val_accuracy`). Make sure you include the axis labels and a legend in each plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z59miDQN743b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,5))\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFydH4WK743b"
      },
      "source": [
        "### Test the model\n",
        "\n",
        "Finally, we evaluate our trained model on the held-out test set. First we predict the outputs for the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjikfSSI743b"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfNszJFH743b"
      },
      "source": [
        "The model output from the final sigmoid layer is a value between 0 and 1 representing the likelihood that a given sample patient has heart disease. To get the predicted classes, we predict 1 if the output was >= 0.5 and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9Dok0Sg743f"
      },
      "outputs": [],
      "source": [
        "pred_classes = [1 if p >= 0.5 else 0 for p in preds]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7JQIF0d743f"
      },
      "source": [
        "Compute and print the test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV09iSDg743f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inpF6nR9743f"
      },
      "source": [
        "**Question 1: There is a large difference between the training and validation accuracies and the test accuracy. What do you think could explain this difference?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning with K-fold Cross Validation"
      ],
      "metadata": {
        "id": "0tJaEbYAOZuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### k-fold Cross validation\n",
        "\n",
        "We will use 5-fold cross validation to train and evaluate our classifier. We will not do any model selection/hyperparameter tuning in this step, so we need to split our data into a training and test set.\n",
        "\n",
        "To split the data into 3 folds we will shuffle the rows and then split them into $k$ equal groups."
      ],
      "metadata": {
        "id": "4cRrLx4h6950"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "\n",
        "# Note: np.split raises error if indices_or_sections is\n",
        "# an integer and doesn't result in equal size splits\n",
        "folds = np.split(data.sample(frac=1, random_state=seed), indices_or_sections=k)"
      ],
      "metadata": {
        "id": "9aHHdAko7B4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a for loop to print the number of samples and number of samples from each class in each fold."
      ],
      "metadata": {
        "id": "kSEWTTrI7Yg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folds[0][folds[0]['target']==0]"
      ],
      "metadata": {
        "id": "t9X-LBKTOz2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n"
      ],
      "metadata": {
        "id": "1GWYeaAR7ZVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a Neural Network classifier\n",
        "\n",
        "We will use the Neural Network (NN) implemented in previous section for our classification model. Use cross validation to train and evaluate the model. Set hyperparameters to `h1_units=32` and `h1_activation='relu'`, and tune the hypaerparameter `learning_rate` with candidate values `[0.01, 0.03]`.\n",
        "\n",
        "Implement a for loop to iterate hyperparameters, and then implement another for loop to iterate through each fold, training a new NN model each iteration with one fold assigned to validation and the remaining folds assigned to training. Compute the validation accuracy for each iteration and append it to the `accuracies` list."
      ],
      "metadata": {
        "id": "i6OnoMnU7btN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def create_model(h1_units, learning_rate, h1_activation):\n",
        "  all_inputs = [\n",
        "    keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\"),\n",
        "    keras.Input(shape=(1,), name=\"thal\", dtype=\"string\"),\n",
        "    keras.Input(shape=(1,), name=\"age\"),\n",
        "    keras.Input(shape=(1,), name=\"trestbps\"),\n",
        "    keras.Input(shape=(1,), name=\"chol\"),\n",
        "    keras.Input(shape=(1,), name=\"thalach\"),\n",
        "    keras.Input(shape=(1,), name=\"oldpeak\"),\n",
        "    keras.Input(shape=(1,), name=\"slope\"),\n",
        "  ]\n",
        "  feature_layer = layers.concatenate(\n",
        "    [\n",
        "        encode_categorical_feature(all_inputs[0], \"sex\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[1], \"cp\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[2], \"fbs\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[3], \"restecg\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[4], \"exang\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[5], \"ca\", train_ds, False),\n",
        "        encode_categorical_feature(all_inputs[6], \"thal\", train_ds, True),\n",
        "        normalize(all_inputs[7], \"age\", train_ds),\n",
        "        normalize(all_inputs[8], \"trestbps\", train_ds),\n",
        "        normalize(all_inputs[9], \"chol\", train_ds),\n",
        "        normalize(all_inputs[10], \"thalach\", train_ds),\n",
        "        normalize(all_inputs[11], \"oldpeak\", train_ds),\n",
        "        normalize(all_inputs[12], \"slope\", train_ds)\n",
        "    ]\n",
        "  )\n",
        "  h1_layer = layers.Dense(h1_units, activation=h1_activation)(feature_layer)\n",
        "  output_layer = layers.Dense(1, activation=\"sigmoid\")(h1_layer)\n",
        "  model = keras.Model(inputs=all_inputs, outputs=output_layer)\n",
        "  model.compile(keras.optimizers.SGD(learning_rate=learning_rate), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "\n",
        "h1_units=32\n",
        "learning_rate=[0.01, 0.03]\n",
        "h1_activation='relu'\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for lr in learning_rate:\n",
        "  accuracies_per_param = []\n",
        "\n",
        "  for i in range(len(folds)):\n",
        "      # assign the folds to training and validation\n",
        "      train = pd.concat(folds[0:i] + folds[i+1:])\n",
        "      val = folds[i]\n",
        "      train_ds_original = dataframe_to_dataset(train)\n",
        "      val_ds_original = dataframe_to_dataset(val, shuffle=False)\n",
        "\n",
        "      train_ds = train_ds_original.batch(32)\n",
        "      val_ds = val_ds_original.batch(val.shape[0])\n",
        "\n",
        "      model = create_model(h1_units, lr, h1_activation)\n",
        "\n",
        "      model_result = model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
        "\n",
        "      # predict test set, the output is a probability, not an integer\n",
        "      pred_val = model.predict(val_ds)\n",
        "\n",
        "      # YOUR CODE HERE\n",
        "      # convert the probability to integer - the class label\n",
        "\n",
        "      # append accuracy to accuracies_per_param list\n",
        "\n",
        "\n",
        "  accuracies.append(accuracies_per_param)"
      ],
      "metadata": {
        "id": "BLl8XH5H9Mch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: How many total combinations of the above hyperparameter choices are there?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "YOUR ANSWER HERE\n",
        "\n"
      ],
      "metadata": {
        "id": "xHYlpFjm9CLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the mean and standard deviation of the accuracy from cross validation for all hyperparams(across all $k$ folds)."
      ],
      "metadata": {
        "id": "07XLoet3-YbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracies)\n",
        "acc_mean = np.mean(accuracies, axis=1)\n",
        "acc_std = np.std(accuracies, axis=1)\n",
        "\n",
        "print(pd.DataFrame.from_dict(\n",
        "    { \"Params\": learning_rate,\n",
        "    \"Accucary Mean\": acc_mean,\n",
        "    \"Accucary Std \": acc_std,\n",
        "}) )\n"
      ],
      "metadata": {
        "id": "ASvPYzph-mr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3: If you increased the number of folds, do you expect the standard deviation of the accuracy across $k$ folds to increase or decrease? Why?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "YOUR ANSWER HERE\n"
      ],
      "metadata": {
        "id": "weFN39Ii-Ygx"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}